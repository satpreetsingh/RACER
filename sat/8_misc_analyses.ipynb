{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['filename', 'subject_id', 'duration'], dtype='object')\n",
      "Index(['filename', 'subject_id', 'num_words', 'num_lines', 'num_characters'], dtype='object')\n",
      "Index(['subject_id', 'PredictedGender'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from utils import ALL_SUBJECT_IDS\n",
    "\n",
    "interview_durations = pd.read_csv(\"../metadata/interview_durations.csv\")\n",
    "transcript_lengths = pd.read_csv(\"../metadata/transcript_lengths.csv\")\n",
    "predicted_gender = pd.read_csv(\"../metadata/predicted_gender_nopii.csv\")\n",
    "predicted_gender[\"PredictedGender\"] = (\n",
    "    predicted_gender[\"PredictedGender\"].str.strip().str.upper()\n",
    ")\n",
    "\n",
    "predicted_gender.rename(columns={\"SubjectID\": \"subject_id\"}, inplace=True)\n",
    "predicted_gender = predicted_gender.query(\"subject_id in @ALL_SUBJECT_IDS\")\n",
    "\n",
    "print(interview_durations.columns)\n",
    "print(transcript_lengths.columns)\n",
    "print(predicted_gender.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictedGender\n",
      "M    51\n",
      "F    42\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(predicted_gender[\"PredictedGender\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripts were on average \n",
      " 22508.54 +/ 7203.33 s.d. characters, \n",
      " 4044.30 +/ 1348.34 s.d. words, and \n",
      " 280.59 +/ 61.50 s.d. lines.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Transcripts were on average \\n {:.2f} +/ {:.2f} s.d. characters, \\n {:.2f} +/ {:.2f} s.d. words, and \\n {:.2f} +/ {:.2f} s.d. lines.\".format(\n",
    "        transcript_lengths[\"num_characters\"].mean(),\n",
    "        transcript_lengths[\"num_characters\"].std(),\n",
    "        transcript_lengths[\"num_words\"].mean(),\n",
    "        transcript_lengths[\"num_words\"].std(),\n",
    "        transcript_lengths[\"num_lines\"].mean(),\n",
    "        transcript_lengths[\"num_lines\"].std(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename subject_id  duration\n",
      "0  C001_audio_only.m4a       C001      2029\n",
      "1  C002_audio_only.m4a       C002      2238\n",
      "2  C003_audio_only.m4a       C003      2156\n",
      "3  C004_audio_only.m4a       C004      1017\n",
      "4  C005_audio_only.m4a       C005      1722\n",
      "(96, 3)\n",
      "93\n",
      "93\n",
      "  subject_id  duration\n",
      "0       C001      2029\n",
      "1       C002      2238\n",
      "2       C003      2156\n",
      "3       C004      1017\n",
      "4       C005      1722\n",
      "Interviews lasted on average 26.7 +/- 8.9 s.d. minutes\n"
     ]
    }
   ],
   "source": [
    "# Note how interview_durations has Part1-Part2 issues\n",
    "print(interview_durations.head(n=5))\n",
    "print(interview_durations.shape)\n",
    "print(interview_durations.subject_id.nunique())\n",
    "# print(interview_durations[\"duration\"].describe())\n",
    "\n",
    "# Summing up durations for each unique subject_id\n",
    "total_durations = (\n",
    "    interview_durations.groupby(\"subject_id\")[\"duration\"].sum().reset_index()\n",
    ")\n",
    "total_durations.columns = [\"subject_id\", \"duration\"]\n",
    "print(len(total_durations))  # Should be 93\n",
    "print(total_durations.head())\n",
    "\n",
    "print(\n",
    "    \"Interviews lasted on average {:.1f} +/- {:.1f} s.d. minutes\".format(\n",
    "        total_durations[\"duration\"].mean() / 60,\n",
    "        total_durations[\"duration\"].std() / 60,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   num_words   num_lines  num_characters\n",
      "PredictedGender                                         \n",
      "F                3832.595238  275.714286     21353.52381\n",
      "M                4218.647059  284.607843     23459.72549\n"
     ]
    }
   ],
   "source": [
    "# Are differences between the transcripts of M/F statistically significant?\n",
    "merged_transcript_data = pd.merge(transcript_lengths, predicted_gender, on=\"subject_id\")\n",
    "assert merged_transcript_data.shape[0] == 93, merged_transcript_data.shape[0]\n",
    "\n",
    "\n",
    "average_metrics_by_gender = merged_transcript_data.groupby(\"PredictedGender\")[\n",
    "    [\"num_words\", \"num_lines\", \"num_characters\"]\n",
    "].mean()\n",
    "\n",
    "print(average_metrics_by_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words - T-statistic: 1.4018535862222636, P-value: 0.16436635207905567\n",
      "num_lines - T-statistic: 0.6891871822932596, P-value: 0.4925582648912449\n",
      "num_characters - T-statistic: 1.4275924098942627, P-value: 0.15685097102515808\n"
     ]
    }
   ],
   "source": [
    "# T-tests for each metric\n",
    "for metric in [\"num_words\", \"num_lines\", \"num_characters\"]:\n",
    "    male_metrics = merged_transcript_data[\n",
    "        merged_transcript_data[\"PredictedGender\"] == \"M\"\n",
    "    ][metric]\n",
    "    female_metrics = merged_transcript_data[\n",
    "        merged_transcript_data[\"PredictedGender\"] == \"F\"\n",
    "    ][metric]\n",
    "    t_stat, p_value = stats.ttest_ind(male_metrics, female_metrics, equal_var=False)\n",
    "    print(f\"{metric} - T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictedGender\n",
      "F    1485.595238\n",
      "M    1701.705882\n",
      "Name: duration, dtype: float64\n",
      "T-statistic: 1.9895127759804625, P-value: 0.04965473728160002\n"
     ]
    }
   ],
   "source": [
    "# Are differences between the durations (seconds) of M/F statistically significant?\n",
    "merged_data = pd.merge(total_durations, predicted_gender, on=\"subject_id\", how=\"inner\")\n",
    "assert merged_data.shape[0] == 93, merged_data.shape[0]\n",
    "# TODO: Why does the above assertion fail?\n",
    "\n",
    "average_durations_by_gender = merged_data.groupby(\"PredictedGender\")[\"duration\"].mean()\n",
    "print(average_durations_by_gender)\n",
    "\n",
    "# T-test for the difference in means\n",
    "male_durations = merged_data[merged_data[\"PredictedGender\"] == \"M\"][\"duration\"]\n",
    "female_durations = merged_data[merged_data[\"PredictedGender\"] == \"F\"][\"duration\"]\n",
    "t_stat, p_value = stats.ttest_ind(male_durations, female_durations, equal_var=False)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
